{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.327776Z",
     "start_time": "2024-09-23T00:43:41.325182Z"
    }
   },
   "source": [
    "from openlrm.models import ModelLRM\n",
    "from openlrm.runners.infer.lrm import LRMInferrer"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.386745Z",
     "start_time": "2024-09-23T00:43:41.383735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CFG:\n",
    "    def __init__(self, image_input='./assets/sample_input/owl.png'):\n",
    "        self.config = \"./configs/infer-b.yaml\"\n",
    "        self.infer = {'lrm': None}\n",
    "        self.model_name = 'zxhezexin/openlrm-mix-base-1.1'\n",
    "        self.image_input = image_input\n",
    "        self.export_video = True\n",
    "        self.export_mesh = True\n",
    "        self.source_size = 336\n",
    "        self.render_size = 288\n",
    "        self.source_cam_dist = 2.0\n",
    "        self.video_dump = 'dumps/zxhezexin/openlrm-mix-base-1.1/videos'\n",
    "        self.mesh_dump = 'dumps/zxhezexin/openlrm-mix-base-1.1/meshes'\n",
    "        self.render_views = 60\n",
    "        self.render_fps = 10\n",
    "        self.mesh_size = 384\n",
    "        self.mesh_thres = 3.0\n",
    "        self.frame_size = 2\n",
    "        self.logger = 'INFO'\n",
    "        self.app_enabled = False"
   ],
   "id": "8bc4a3d9a38c273a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.434202Z",
     "start_time": "2024-09-23T00:43:41.431585Z"
    }
   },
   "cell_type": "code",
   "source": "cfg = CFG('./assets/sample_input/pawn.jpg')",
   "id": "a4cc526cc6d70087",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.487307Z",
     "start_time": "2024-09-23T00:43:41.482589Z"
    }
   },
   "cell_type": "code",
   "source": "cfg",
   "id": "2e5c1478538cb9fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CFG at 0x7f851afaad90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.541488Z",
     "start_time": "2024-09-23T00:43:41.537406Z"
    }
   },
   "cell_type": "code",
   "source": "cfg",
   "id": "a7f84256db8726a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CFG at 0x7f851afaad90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.597411Z",
     "start_time": "2024-09-23T00:43:41.594821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# {'source_size': 336, 'source_cam_dist': 2.0, 'render_size': 288, 'render_views': 160, 'render_fps': 40, 'frame_size': 2, 'mesh_size': 384, 'mesh_thres': 3.0, 'video_dump': 'dumps/zxhezexin/openlrm-mix-\n",
    "# base-1.1/videos', 'mesh_dump': 'dumps/zxhezexin/openlrm-mix-base-1.1/meshes', 'infer': {'lrm': None}, 'model_name': 'zxhezexin/openlrm-mix-base-1.1', 'image_input': './assets/sample_input/owl.png', 'export_video': True, 'export_mesh': True, 'logger': 'INFO', 'app_enabled': False}\n"
   ],
   "id": "4957de022adc02d8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.643213Z",
     "start_time": "2024-09-23T00:43:41.640258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# {'experiment': {'type': 'lrm', 'seed': 42, 'parent': 'lrm-objaverse', 'child': 'small-dummyrun'}, 'model': {'camera_embed_dim': 1024, 'rendering_samples_per_ray': 96, 'transformer_dim': 512, 'transform\n",
    "# er_layers': 12, 'transformer_heads': 8, 'triplane_low_res': 32, 'triplane_high_res': 64, 'triplane_dim': 32, 'encoder_type': 'dinov2', 'encoder_model_name': 'dinov2_vits14_reg', 'encoder_feat_dim': 384\n",
    "# , 'encoder_freeze': False}, 'dataset': {'subsets': [{'name': 'objaverse', 'root_dirs': ['<REPLACE_WITH_RENDERING_ROOT>'], 'meta_path': {'train': '<TRAIN_UIDS_IN_JSON>', 'val': '<VAL_UIDS_IN_JSON>'}, 's\n",
    "# ample_rate': 1.0}], 'sample_side_views': 3, 'source_image_res': 224, 'render_image': {'low': 64, 'high': 192, 'region': 64}, 'normalize_camera': True, 'normed_dist_to_center': 'auto', 'num_train_worker\n",
    "# s': 4, 'num_val_workers': 2, 'pin_mem': True}, 'train': {'mixed_precision': 'bf16', 'find_unused_parameters': False, 'loss': {'pixel_weight': 1.0, 'perceptual_weight': 1.0, 'tv_weight': 0.0005}, 'optim\n",
    "# ': {'lr': 0.0004, 'weight_decay': 0.05, 'beta1': 0.9, 'beta2': 0.95, 'clip_grad_norm': 1.0}, 'scheduler': {'type': 'cosine', 'warmup_real_iters': 3000}, 'batch_size': 16, 'accum_steps': 1, 'epochs': 60\n",
    "# , 'debug_global_steps': None, 'lrm': None}, 'val': {'batch_size': 4, 'global_step_period': 1000, 'debug_batches': None}, 'saver': {'auto_resume': True, 'load_model': None, 'checkpoint_root': './exps/ch\n",
    "# eckpoints', 'checkpoint_global_steps': 1000, 'checkpoint_keep_level': 5}, 'logger': {'stream_level': 'WARNING', 'log_level': 'INFO', 'log_root': './exps/logs', 'tracker_root': './exps/trackers', 'enabl\n",
    "# e_profiler': False, 'trackers': ['tensorboard'], 'image_monitor': {'train_global_steps': 100, 'samples_per_log': 4}}, 'compile': {'suppress_errors': True, 'print_specializations': True, 'disable': True}}"
   ],
   "id": "6ff24bc237e02165",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.693399Z",
     "start_time": "2024-09-23T00:43:41.690363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import mcubes\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "from openlrm.runners.infer.base_inferrer import Inferrer\n",
    "from openlrm.datasets.cam_utils import build_camera_principle, build_camera_standard, surrounding_views_linspace, create_intrinsics\n",
    "from openlrm.utils.logging import configure_logger\n",
    "from openlrm.runners import REGISTRY_RUNNERS\n",
    "from openlrm.utils.video import images_to_video\n",
    "from openlrm.utils.hf_hub import wrap_model_hub\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)"
   ],
   "id": "e07e92fe503f5bc2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.744353Z",
     "start_time": "2024-09-23T00:43:41.740848Z"
    }
   },
   "cell_type": "code",
   "source": "np.__version__",
   "id": "afa89f7dcd3a7ef7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.810158Z",
     "start_time": "2024-09-23T00:43:41.806075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import openlrm\n",
    "\n",
    "openlrm.__file__"
   ],
   "id": "8caa3a8068768d21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/opnlrm_real/OpenLRM/openlrm/__init__.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.879782Z",
     "start_time": "2024-09-23T00:43:41.865861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRMInferrer(Inferrer):\n",
    "\n",
    "    EXP_TYPE: str = 'lrm'\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        configure_logger(\n",
    "            stream_level=self.cfg.logger,\n",
    "            log_level=self.cfg.logger,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        self.model = self._build_model(self.cfg).to(self.device)\n",
    "\n",
    "    def _build_model(self, cfg):\n",
    "        from openlrm.models import model_dict\n",
    "        hf_model_cls = wrap_model_hub(model_dict[self.EXP_TYPE])\n",
    "        model = hf_model_cls.from_pretrained(cfg.model_name)\n",
    "        return model\n",
    "\n",
    "    def _default_source_camera(self, dist_to_center: float = 2.0, batch_size: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        # return: (N, D_cam_raw)\n",
    "        canonical_camera_extrinsics = torch.tensor([[\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 0, -1, -dist_to_center],\n",
    "            [0, 1, 0, 0],\n",
    "        ]], dtype=torch.float32, device=device)\n",
    "        canonical_camera_intrinsics = create_intrinsics(\n",
    "            f=0.75,\n",
    "            c=0.5,\n",
    "            device=device,\n",
    "        ).unsqueeze(0)\n",
    "        source_camera = build_camera_principle(canonical_camera_extrinsics, canonical_camera_intrinsics)\n",
    "        return source_camera.repeat(batch_size, 1)\n",
    "\n",
    "    def _default_render_cameras(self, n_views: int, batch_size: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        # return: (N, M, D_cam_render)\n",
    "        render_camera_extrinsics = surrounding_views_linspace(n_views=n_views, device=device)\n",
    "        render_camera_intrinsics = create_intrinsics(\n",
    "            f=0.75,\n",
    "            c=0.5,\n",
    "            device=device,\n",
    "        ).unsqueeze(0).repeat(render_camera_extrinsics.shape[0], 1, 1)\n",
    "        render_cameras = build_camera_standard(render_camera_extrinsics, render_camera_intrinsics)\n",
    "        return render_cameras.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    def infer_planes(self, image: torch.Tensor, source_cam_dist: float):\n",
    "        N = image.shape[0]\n",
    "        source_camera = self._default_source_camera(dist_to_center=source_cam_dist, batch_size=N, device=self.device)\n",
    "\n",
    "        planes = self.model.forward_planes(image, source_camera)\n",
    "\n",
    "        assert N == planes.shape[0]\n",
    "        return planes\n",
    "\n",
    "    def infer_video(self, planes: torch.Tensor, frame_size: int, render_size: int, render_views: int, render_fps: int, dump_video_path: str):\n",
    "        N = planes.shape[0]\n",
    "        render_cameras = self._default_render_cameras(n_views=render_views, batch_size=N, device=self.device)\n",
    "        render_anchors = torch.zeros(N, render_cameras.shape[1], 2, device=self.device)\n",
    "        render_resolutions = torch.ones(N, render_cameras.shape[1], 1, device=self.device) * render_size\n",
    "        render_bg_colors = torch.ones(N, render_cameras.shape[1], 1, device=self.device, dtype=torch.float32) * 1.\n",
    "\n",
    "        frames = []\n",
    "        for i in range(0, render_cameras.shape[1], frame_size):\n",
    "            frames.append(\n",
    "                self.model.synthesizer(\n",
    "                    planes=planes,\n",
    "                    cameras=render_cameras[:, i:i+frame_size],\n",
    "                    anchors=render_anchors[:, i:i+frame_size],\n",
    "                    resolutions=render_resolutions[:, i:i+frame_size],\n",
    "                    bg_colors=render_bg_colors[:, i:i+frame_size],\n",
    "                    region_size=render_size,\n",
    "                )\n",
    "            )\n",
    "        # merge frames\n",
    "        frames = {\n",
    "            k: torch.cat([r[k] for r in frames], dim=1)\n",
    "            for k in frames[0].keys()\n",
    "        }\n",
    "        # dump\n",
    "        os.makedirs(os.path.dirname(dump_video_path), exist_ok=True)\n",
    "        for k, v in frames.items():\n",
    "            if k == 'images_rgb':\n",
    "                images_to_video(\n",
    "                    images=v[0],\n",
    "                    output_path=dump_video_path,\n",
    "                    fps=render_fps,\n",
    "                    gradio_codec=self.cfg.app_enabled,\n",
    "                )\n",
    "\n",
    "    def infer_mesh(self, planes: torch.Tensor, mesh_size: int, mesh_thres: float, dump_mesh_path: str):\n",
    "        grid_out = self.model.synthesizer.forward_grid(\n",
    "            planes=planes,\n",
    "            grid_size=mesh_size,\n",
    "        )\n",
    "        \n",
    "        vtx, faces = mcubes.marching_cubes(grid_out['sigma'].squeeze(0).squeeze(-1).cpu().numpy(), mesh_thres)\n",
    "        vtx = vtx / (mesh_size - 1) * 2 - 1\n",
    "\n",
    "        vtx_tensor = torch.tensor(vtx, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        vtx_colors = self.model.synthesizer.forward_points(planes, vtx_tensor)['rgb'].squeeze(0).cpu().numpy()  # (0, 1)\n",
    "        vtx_colors = (vtx_colors * 255).astype(np.uint8)\n",
    "        \n",
    "        mesh = trimesh.Trimesh(vertices=vtx, faces=faces, vertex_colors=vtx_colors)\n",
    "\n",
    "        # dump\n",
    "        os.makedirs(os.path.dirname(dump_mesh_path), exist_ok=True)\n",
    "        mesh.export(dump_mesh_path)\n",
    "\n",
    "    def infer_single(self, image_path: str, source_cam_dist: float, export_video: bool, export_mesh: bool, dump_video_path: str, dump_mesh_path: str):\n",
    "        source_size = self.cfg.source_size\n",
    "        render_size = self.cfg.render_size\n",
    "        render_views = self.cfg.render_views\n",
    "        render_fps = self.cfg.render_fps\n",
    "        mesh_size = self.cfg.mesh_size\n",
    "        mesh_thres = self.cfg.mesh_thres\n",
    "        frame_size = self.cfg.frame_size\n",
    "        source_cam_dist = self.cfg.source_cam_dist if source_cam_dist is None else source_cam_dist\n",
    "\n",
    "        # prepare image: [1, C_img, H_img, W_img], 0-1 scale\n",
    "        image = torch.from_numpy(np.array(Image.open(image_path))).to(self.device)\n",
    "        image = image.permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "        if image.shape[1] == 4:  # RGBA\n",
    "            image = image[:, :3, ...] * image[:, 3:, ...] + (1 - image[:, 3:, ...])\n",
    "        image = torch.nn.functional.interpolate(image, size=(source_size, source_size), mode='bicubic', align_corners=True)\n",
    "        image = torch.clamp(image, 0, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            planes = self.infer_planes(image, source_cam_dist=source_cam_dist)\n",
    "\n",
    "            results = {}\n",
    "            if export_video:\n",
    "                frames = self.infer_video(planes, frame_size=frame_size, render_size=render_size, render_views=render_views, render_fps=render_fps, dump_video_path=dump_video_path)\n",
    "                results.update({\n",
    "                    'frames': frames,\n",
    "                })\n",
    "            if export_mesh:\n",
    "                mesh = self.infer_mesh(planes, mesh_size=mesh_size, mesh_thres=mesh_thres, dump_mesh_path=dump_mesh_path)\n",
    "                results.update({\n",
    "                    'mesh': mesh,\n",
    "                })\n",
    "\n",
    "    def infer(self):\n",
    "\n",
    "        image_paths = []\n",
    "        if os.path.isfile(self.cfg.image_input):\n",
    "            omit_prefix = os.path.dirname(self.cfg.image_input)\n",
    "            image_paths.append(self.cfg.image_input)\n",
    "        else:\n",
    "            omit_prefix = self.cfg.image_input\n",
    "            for root, dirs, files in os.walk(self.cfg.image_input):\n",
    "                for file in files:\n",
    "                    if file.endswith('.png'):\n",
    "                        image_paths.append(os.path.join(root, file))\n",
    "            image_paths.sort()\n",
    "\n",
    "        # alloc to each DDP worker\n",
    "        image_paths = image_paths[self.accelerator.process_index::self.accelerator.num_processes]\n",
    "\n",
    "        for image_path in tqdm(image_paths, disable=not self.accelerator.is_local_main_process):\n",
    "\n",
    "            # prepare dump paths\n",
    "            image_name = os.path.basename(image_path)\n",
    "            uid = image_name.split('.')[0]\n",
    "            subdir_path = os.path.dirname(image_path).replace(omit_prefix, '')\n",
    "            subdir_path = subdir_path[1:] if subdir_path.startswith('/') else subdir_path\n",
    "            dump_video_path = os.path.join(\n",
    "                self.cfg.video_dump,\n",
    "                subdir_path,\n",
    "                f'{uid}.mov',\n",
    "            )\n",
    "            dump_mesh_path = os.path.join(\n",
    "                self.cfg.mesh_dump,\n",
    "                subdir_path,\n",
    "                f'{uid}.ply',\n",
    "            )\n",
    "\n",
    "            self.infer_single(\n",
    "                image_path,\n",
    "                source_cam_dist=None,\n",
    "                export_video=self.cfg.export_video,\n",
    "                export_mesh=self.cfg.export_mesh,\n",
    "                dump_video_path=dump_video_path,\n",
    "                dump_mesh_path=dump_mesh_path,\n",
    "            )"
   ],
   "id": "f5677a9b1c6137b1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:41.947047Z",
     "start_time": "2024-09-23T00:43:41.944202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class ModelLRMv2(ModelLRM):\n",
    "#     def __init__(self, camera_embed_dim: int, rendering_samples_per_ray: int,\n",
    "#                  transformer_dim: int, transformer_layers: int, transformer_heads: int,\n",
    "#                  triplane_low_res: int, triplane_high_res: int, triplane_dim: int,\n",
    "#                  encoder_freeze: bool = True, encoder_type: str = 'dino',\n",
    "#                  encoder_model_name: str = 'facebook/dino-vitb16', encoder_feat_dim: int = 768):\n",
    "#         super().__init__(camera_embed_dim, rendering_samples_per_ray,\n",
    "#                  transformer_dim, transformer_layers, transformer_heads,\n",
    "#                  triplane_low_res, triplane_high_res, triplane_dim,\n",
    "#                  encoder_freeze, encoder_type, encoder_model_name, encoder_feat_dim)\n",
    "#         self.model = self._build_model(self.cfg).to(self.device)\n",
    "\n",
    "    \n",
    "    "
   ],
   "id": "f9402b32e7718138",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:46.750796Z",
     "start_time": "2024-09-23T00:43:41.991334Z"
    }
   },
   "cell_type": "code",
   "source": "lrm_i = LRMInferrer()",
   "id": "23ab16fdcf72e247",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-22 20:43:42,894] openlrm.models.modeling_lrm: [INFO] Using DINOv2 as the encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/opnlrm_real/OpenLRM/openlrm/models/encoders/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/opnlrm_real/OpenLRM/openlrm/models/encoders/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/opnlrm_real/OpenLRM/openlrm/models/encoders/dinov2/layers/block.py:46: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:47.112570Z",
     "start_time": "2024-09-23T00:43:46.968239Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import BertModel, BertTokenizer",
   "id": "1959e02d6a0cd541",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:47.999255Z",
     "start_time": "2024-09-23T00:43:47.134375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased').to(\"cuda\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "id": "b7a07bb11071bf7e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinsonunix/miniconda3/envs/openlrm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:48.018390Z",
     "start_time": "2024-09-23T00:43:48.015428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"pawn\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")"
   ],
   "id": "a07b9b7482a9d1b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:48.396033Z",
     "start_time": "2024-09-23T00:43:48.080358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs\n",
    "bert(**inputs)"
   ],
   "id": "253b4df547c942c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.5961,  0.0817, -0.0259,  ..., -0.1485,  0.0682,  1.1705],\n",
       "         [-0.6663, -0.8367,  0.4010,  ...,  0.3661, -0.0528,  0.3260],\n",
       "         [ 0.9318,  0.0092, -0.3196,  ...,  0.1259, -0.8594, -0.1575]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9324, -0.3545, -0.0097,  0.8321,  0.1828, -0.2639,  0.9565,  0.1587,\n",
       "         -0.3545, -1.0000, -0.4712,  0.7049,  0.9891,  0.0572,  0.9486, -0.6808,\n",
       "         -0.4614, -0.5864,  0.5341, -0.9010,  0.7179,  0.9990,  0.4551,  0.3323,\n",
       "          0.5417,  0.9128, -0.6911,  0.9471,  0.9690,  0.7658, -0.7925,  0.2185,\n",
       "         -0.9921, -0.3486, -0.4658, -0.9930,  0.3867, -0.8437, -0.1463, -0.0235,\n",
       "         -0.9416,  0.4951,  0.9999,  0.4803,  0.4872, -0.4367, -1.0000,  0.3117,\n",
       "         -0.9295,  0.4664,  0.3824,  0.1156,  0.2471,  0.5555,  0.6331,  0.0898,\n",
       "          0.0425,  0.2780, -0.3225, -0.6382, -0.6747,  0.4071, -0.4357, -0.9256,\n",
       "          0.3615, -0.0656, -0.2088, -0.3537, -0.0831,  0.1188,  0.8766,  0.2861,\n",
       "          0.3402, -0.8180, -0.0869,  0.2644, -0.6188,  1.0000, -0.7477, -0.9873,\n",
       "          0.2846, -0.0244,  0.5927,  0.3936, -0.2297, -1.0000,  0.5446, -0.1640,\n",
       "         -0.9932,  0.2864,  0.6402, -0.3569, -0.1777,  0.5991, -0.2952, -0.4369,\n",
       "         -0.4165, -0.4910, -0.3735, -0.3303,  0.2757, -0.2890, -0.2631, -0.4705,\n",
       "          0.4199, -0.4328, -0.6222,  0.4651, -0.2203,  0.7932,  0.3221, -0.3846,\n",
       "          0.5139, -0.9667,  0.6988, -0.3686, -0.9907, -0.5650, -0.9930,  0.8059,\n",
       "         -0.3589, -0.2950,  0.9665,  0.1741,  0.3407,  0.0282, -0.4207, -1.0000,\n",
       "         -0.5307, -0.6134,  0.1880, -0.2779, -0.9855, -0.9728,  0.7232,  0.9747,\n",
       "          0.2264,  0.9996, -0.3673,  0.9422, -0.0299, -0.4206,  0.0145, -0.4622,\n",
       "          0.6833,  0.6870, -0.8429,  0.3426, -0.0984, -0.0022, -0.4070, -0.3486,\n",
       "         -0.2369, -0.9489, -0.4508,  0.9653,  0.0019, -0.3716,  0.6174, -0.2919,\n",
       "         -0.5631,  0.9127,  0.6404,  0.4409, -0.1079,  0.5525, -0.0587,  0.5874,\n",
       "         -0.9110,  0.2285,  0.4447, -0.3334, -0.2459, -0.9901, -0.4951,  0.4869,\n",
       "          0.9922,  0.7819,  0.3195,  0.5163, -0.3269,  0.4564, -0.9624,  0.9878,\n",
       "         -0.3442,  0.2958, -0.1581, -0.1737, -0.9335, -0.3159,  0.8983, -0.2773,\n",
       "         -0.9029, -0.0212, -0.5904, -0.4983, -0.3085,  0.6136, -0.3826, -0.5047,\n",
       "         -0.2629,  0.9335,  0.9887,  0.8545, -0.2056,  0.7238, -0.9201, -0.5831,\n",
       "          0.1593,  0.4091,  0.2999,  0.9962, -0.1736, -0.2383, -0.9376, -0.9878,\n",
       "          0.2386, -0.9228, -0.0496, -0.8024,  0.4798,  0.3603, -0.0339,  0.5619,\n",
       "         -0.9946, -0.8278,  0.4518, -0.5262,  0.4704, -0.3064,  0.5438,  0.5265,\n",
       "         -0.6613,  0.9137,  0.9154, -0.0357, -0.8436,  0.9187, -0.3870,  0.8955,\n",
       "         -0.6838,  0.9913,  0.4148,  0.6714, -0.9531,  0.1229, -0.9599, -0.0269,\n",
       "         -0.3271, -0.5370,  0.2946,  0.5989,  0.4141,  0.6801, -0.6075,  0.9989,\n",
       "         -0.5607, -0.9734,  0.1381, -0.3800, -0.9947,  0.6134,  0.3287,  0.0765,\n",
       "         -0.5360, -0.4977, -0.9801,  0.9613,  0.1789,  0.9955, -0.0589, -0.9763,\n",
       "         -0.4130, -0.9467, -0.0729, -0.2555,  0.3001, -0.0592, -0.9703,  0.5864,\n",
       "          0.6441,  0.4859, -0.0163,  0.9990,  1.0000,  0.9820,  0.8912,  0.9097,\n",
       "         -0.9916, -0.0338,  1.0000, -0.9272, -1.0000, -0.9624, -0.6420,  0.5442,\n",
       "         -1.0000, -0.1057, -0.0896, -0.9476,  0.1515,  0.9850,  0.9965, -1.0000,\n",
       "          0.8513,  0.9526, -0.7078,  0.7390, -0.2829,  0.9827,  0.5201,  0.4819,\n",
       "         -0.2826,  0.4409, -0.5483, -0.9295, -0.2242, -0.1160,  0.9305,  0.2722,\n",
       "         -0.8178, -0.9258,  0.1548, -0.1774, -0.0347, -0.9655, -0.2884, -0.2181,\n",
       "          0.7779,  0.2016,  0.2945, -0.7252,  0.3336, -0.3015,  0.5042,  0.6337,\n",
       "         -0.9253, -0.7106, -0.7001, -0.3665,  0.0294, -0.9495,  0.9786, -0.3769,\n",
       "          0.4015,  1.0000,  0.3058, -0.9104,  0.4506,  0.4599, -0.7151,  1.0000,\n",
       "          0.6909, -0.9875, -0.5270,  0.3656, -0.5060, -0.5104,  0.9992, -0.3341,\n",
       "         -0.1977,  0.1693,  0.9881, -0.9956,  0.8419, -0.9178, -0.9710,  0.9764,\n",
       "          0.9660, -0.3289, -0.7229,  0.1597, -0.1871,  0.4478, -0.9691,  0.7704,\n",
       "          0.6581, -0.3025,  0.9363, -0.9370, -0.5914,  0.5069, -0.0743,  0.3832,\n",
       "          0.5055,  0.5693, -0.4061,  0.0943, -0.3573, -0.2770, -0.9654,  0.3165,\n",
       "          1.0000,  0.1036,  0.2994, -0.0513, -0.2122, -0.2034,  0.5251,  0.5347,\n",
       "         -0.3471, -0.8177,  0.4313, -0.9696, -0.9914,  0.8353,  0.3090, -0.4446,\n",
       "          1.0000,  0.4516,  0.3459, -0.1556,  0.8611,  0.0805,  0.4887,  0.0938,\n",
       "          0.9845, -0.3613,  0.5520,  0.8869, -0.4038, -0.3921, -0.7335,  0.1455,\n",
       "         -0.9588, -0.1083, -0.9684,  0.9818,  0.4396,  0.4016,  0.3053,  0.4361,\n",
       "          1.0000, -0.0361,  0.7482, -0.8904,  0.9550, -0.9900, -0.8187, -0.3616,\n",
       "         -0.2530, -0.0966, -0.3447,  0.4259, -0.9859,  0.2004,  0.1530, -0.9909,\n",
       "         -0.9935,  0.3195,  0.8862,  0.1789, -0.8260, -0.7539, -0.6631,  0.3302,\n",
       "         -0.3653, -0.9638,  0.3884, -0.2906,  0.6478, -0.3428,  0.5814,  0.1590,\n",
       "          0.7808, -0.0961,  0.2102, -0.0605, -0.8803,  0.8960, -0.9154, -0.3586,\n",
       "         -0.2931,  1.0000, -0.5599,  0.4739,  0.7936,  0.7743, -0.3421,  0.3073,\n",
       "          0.5427,  0.2900,  0.0580, -0.0978, -0.9458, -0.3851,  0.6344,  0.0728,\n",
       "         -0.2873,  0.8732,  0.4984,  0.2169, -0.0585,  0.1944,  0.9997, -0.4082,\n",
       "         -0.1707, -0.5892, -0.2888, -0.3302, -0.8050,  1.0000,  0.4417,  0.0416,\n",
       "         -0.9926, -0.1535, -0.9471,  1.0000,  0.8563, -0.8654,  0.6058,  0.5200,\n",
       "         -0.1514,  0.8883, -0.3963, -0.3902,  0.1759,  0.1106,  0.9692, -0.5494,\n",
       "         -0.9805, -0.6331,  0.4983, -0.9630,  0.9933, -0.6391, -0.2710, -0.5097,\n",
       "          0.2601,  0.9590,  0.1463, -0.9824, -0.3301,  0.2367,  0.9814,  0.3721,\n",
       "         -0.5983, -0.9489,  0.1666,  0.3674, -0.1607, -0.9401,  0.9798, -0.9857,\n",
       "          0.5968,  1.0000,  0.4306, -0.4129,  0.3301, -0.6071,  0.2407, -0.4159,\n",
       "          0.7373, -0.9680, -0.3959, -0.2799,  0.3202, -0.3578,  0.2672,  0.6939,\n",
       "          0.2158, -0.5744, -0.5861, -0.1845,  0.4576,  0.8398, -0.3605, -0.2283,\n",
       "          0.2100, -0.0956, -0.9314, -0.2343, -0.3642, -0.9995,  0.6381, -1.0000,\n",
       "         -0.0112, -0.5620, -0.3368,  0.8908,  0.1758,  0.2319, -0.8093, -0.1412,\n",
       "          0.6916,  0.8435, -0.4783, -0.0944, -0.7874,  0.3139, -0.1676,  0.2995,\n",
       "         -0.2618,  0.7477, -0.2016,  1.0000,  0.2495, -0.6426, -0.9888,  0.3462,\n",
       "         -0.4307,  1.0000, -0.9400, -0.9701,  0.3289, -0.6096, -0.8473,  0.2921,\n",
       "          0.1254, -0.6823, -0.7600,  0.9563,  0.9450, -0.4395,  0.5975, -0.3370,\n",
       "         -0.5834,  0.1932,  0.2324,  0.9914,  0.4921,  0.9514,  0.7229, -0.1370,\n",
       "          0.9676,  0.2892,  0.7571,  0.2737,  1.0000,  0.4325, -0.9591, -0.1725,\n",
       "         -0.9882, -0.3550, -0.9761,  0.2931,  0.2937,  0.9313, -0.4988,  0.9754,\n",
       "         -0.0076,  0.1683, -0.2383,  0.1033,  0.3956, -0.9410, -0.9896, -0.9934,\n",
       "          0.5413, -0.6040, -0.1352,  0.3157,  0.2430,  0.5058,  0.4500, -1.0000,\n",
       "          0.9531,  0.5772,  0.2125,  0.9728,  0.3307,  0.4786,  0.4371, -0.9930,\n",
       "         -0.9873, -0.4561, -0.3180,  0.8796,  0.6936,  0.8964,  0.4943, -0.5348,\n",
       "         -0.3056, -0.1360, -0.0954, -0.9948,  0.4692,  0.0483, -0.9840,  0.9692,\n",
       "         -0.4843, -0.4192,  0.5553, -0.2163,  0.9670,  0.7889,  0.7297,  0.2566,\n",
       "          0.6435,  0.9288,  0.9724,  0.9931, -0.3277,  0.8360,  0.0263,  0.5059,\n",
       "          0.5855, -0.9663,  0.1500,  0.2285, -0.2968,  0.4655, -0.3677, -0.9771,\n",
       "          0.8239, -0.3425,  0.5651, -0.4765,  0.0951, -0.6221, -0.4504, -0.8438,\n",
       "         -0.6129,  0.6217,  0.5335,  0.9593,  0.3417, -0.0935, -0.8021, -0.2933,\n",
       "          0.0116, -0.9614,  0.9654, -0.2236,  0.3901,  0.2529,  0.1115,  0.7508,\n",
       "         -0.2852, -0.4887, -0.3853, -0.8641,  0.8198, -0.2132, -0.6233, -0.6708,\n",
       "          0.6467,  0.4509,  0.9993, -0.1348, -0.2483, -0.2758, -0.2746,  0.3204,\n",
       "         -0.5279, -1.0000,  0.5363,  0.1909,  0.1016, -0.5145,  0.2344, -0.1239,\n",
       "         -0.9888, -0.3627,  0.2417,  0.3488, -0.5434, -0.6195,  0.5689,  0.5134,\n",
       "          0.8036,  0.9080, -0.2536,  0.5569,  0.6357, -0.1188, -0.7028,  0.9461]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:48.408204Z",
     "start_time": "2024-09-23T00:43:48.404904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.compile\n",
    "def forward_planes(image, camera):\n",
    "    # image: [N, C_img, H_img, W_img]\n",
    "    # camera: [N, D_cam_raw]\n",
    "    \n",
    "    N = image.shape[0]\n",
    "    \n",
    "    # encode image\n",
    "    image_feats = lrm_i.model.encoder(image)\n",
    "    assert image_feats.shape[-1] == lrm_i.model.encoder_feat_dim, \\\n",
    "        f\"Feature dimension mismatch: {image_feats.shape[-1]} vs {lrm_i.model.encoder_feat_dim}\"\n",
    "\n",
    "    # embed camera\n",
    "    camera_embeddings = lrm_i.model.camera_embedder(camera)\n",
    "    assert camera_embeddings.shape[-1] == lrm_i.model.camera_embed_dim, \\\n",
    "        f\"Feature dimension mismatch: {camera_embeddings.shape[-1]} vs {lrm_i.model.camera_embed_dim}\"\n",
    "    \n",
    "    text_feats = bert(**inputs).last_hidden_state\n",
    "    # transformer generating planes\n",
    "    feats = image_feats/2 + text_feats[:,0,:]/2\n",
    "    \n",
    "\n",
    "    tokens = lrm_i.model.forward_transformer(feats, camera_embeddings)\n",
    "    planes = lrm_i.model.reshape_upsample(tokens)\n",
    "    assert planes.shape[0] == N, \"Batch size mismatch for planes\"\n",
    "    assert planes.shape[1] == 3, \"Planes should have 3 channels\"\n",
    "\n",
    "    return planes"
   ],
   "id": "d939638d6e72b7f1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:43:48.462947Z",
     "start_time": "2024-09-23T00:43:48.460589Z"
    }
   },
   "cell_type": "code",
   "source": "lrm_i.model.forward_planes = forward_planes",
   "id": "7706fa73af8b2a6c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:48:00.425539Z",
     "start_time": "2024-09-23T00:43:48.513913Z"
    }
   },
   "cell_type": "code",
   "source": "lrm_i.infer()",
   "id": "1ec29c4212260e95",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [04:11<00:00, 251.91s/it]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T00:48:00.484273Z",
     "start_time": "2024-09-23T00:48:00.482498Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9ccb44ccc377c19c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
