{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:21.626118Z",
     "start_time": "2024-09-26T18:44:18.035397Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "#from .base_trainer import Trainer\n",
    "from openlrm.runners.train.base_trainer import Trainer\n",
    "from openlrm.utils.profiler import DummyProfiler\n",
    "from openlrm.runners import REGISTRY_RUNNERS\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import shutil\n",
    "import torch\n",
    "import safetensors\n",
    "from omegaconf import OmegaConf\n",
    "from abc import abstractmethod\n",
    "from contextlib import contextmanager\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "\n",
    "from openlrm.utils.logging import configure_logger\n",
    "from openlrm.utils.compile import configure_dynamo\n",
    "from openlrm.runners.abstract import Runner\n",
    "import logging\n",
    "from transformers import BertModel, BertTokenizer\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinsonunix/miniconda3/envs/openlrm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:21.638905Z",
     "start_time": "2024-09-26T18:44:21.635983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttributeDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ],
   "id": "b4a0e334370f33b8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.023532Z",
     "start_time": "2024-09-26T18:44:22.021942Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "64242e568c823eca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.097212Z",
     "start_time": "2024-09-26T18:44:22.091419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logger = get_logger(__name__)\n",
    "class CFG:\n",
    "    def __init__(self):\n",
    "        self.experiment = AttributeDict({\n",
    "            'type': 'lrm',\n",
    "            'seed': 42,\n",
    "            'parent': 'lrm-objaverse',\n",
    "            'child': 'small-dummyrun'\n",
    "        })\n",
    "        self.model = AttributeDict({\n",
    "            'camera_embed_dim': 1024,\n",
    "            'rendering_samples_per_ray': 96,\n",
    "            'transformer_dim': 512,\n",
    "            'transformer_layers': 12,\n",
    "            'transformer_heads': 8,\n",
    "            'triplane_low_res': 32,\n",
    "            'triplane_high_res': 64,\n",
    "            'triplane_dim': 32,\n",
    "            'encoder_type': 'dinov2',\n",
    "            'encoder_model_name': 'dinov2_vits14_reg',\n",
    "            'encoder_feat_dim': 384,\n",
    "            'encoder_freeze': False\n",
    "        })\n",
    "        self.dataset = AttributeDict({\n",
    "            'subsets': [\n",
    "                AttributeDict({\n",
    "                    'name': 'objaverse',\n",
    "                    'root_dirs': ['/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/OpenLRM/scripts/data/objaverse/views'],\n",
    "                    'meta_path': AttributeDict({\n",
    "                        'train': '/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/OpenLRM/scripts/data/lol.json',\n",
    "                        'val': '/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/OpenLRM/scripts/data/lol.json'\n",
    "                    }),\n",
    "                    'sample_rate': 1.0\n",
    "                })\n",
    "            ],\n",
    "            'sample_side_views': 3,\n",
    "            'source_image_res': 224,\n",
    "            'render_image': AttributeDict({\n",
    "                'low': 64,\n",
    "                'high': 192,\n",
    "                'region': 64\n",
    "            }),\n",
    "            'normalize_camera': True,\n",
    "            'normed_dist_to_center': 'auto',\n",
    "            'num_train_workers': 4,\n",
    "            'num_val_workers': 2,\n",
    "            'pin_mem': True\n",
    "        })\n",
    "        self.train = AttributeDict({\n",
    "            'mixed_precision': 'no',\n",
    "            'find_unused_parameters': False,\n",
    "            'loss': AttributeDict({\n",
    "                'pixel_weight': 1.0,\n",
    "                'perceptual_weight': 1.0,\n",
    "                'tv_weight': 0.0005\n",
    "            }),\n",
    "            'optim': AttributeDict({\n",
    "                'lr': 0.0004,\n",
    "                'weight_decay': 0.05,\n",
    "                'beta1': 0.9,\n",
    "                'beta2': 0.95,\n",
    "                'clip_grad_norm': 1.0\n",
    "            }),\n",
    "            'scheduler': AttributeDict({\n",
    "                'type': 'cosine',\n",
    "                'warmup_real_iters': 3000\n",
    "            }),\n",
    "            'batch_size': 1,\n",
    "            'accum_steps': 1,\n",
    "            'epochs': 60,\n",
    "            'debug_global_steps': None,\n",
    "            'lrm': None\n",
    "        })\n",
    "        self.val = AttributeDict({\n",
    "            'batch_size': 4,\n",
    "            'global_step_period': 1000,\n",
    "            'debug_batches': None\n",
    "        })\n",
    "        self.saver = AttributeDict({\n",
    "            'auto_resume': True,\n",
    "            'load_model': None,\n",
    "            'checkpoint_root': './exps/checkpoints',\n",
    "            'checkpoint_global_steps': 1000,\n",
    "            'checkpoint_keep_level': 5\n",
    "        })\n",
    "        self.logger = AttributeDict({\n",
    "            'stream_level': 'WARNING',\n",
    "            'log_level': 'INFO',\n",
    "            'log_root': './exps/logs',\n",
    "            'tracker_root': './exps/trackers',\n",
    "            'enable_profiler': False,\n",
    "            'trackers': ['tensorboard'],\n",
    "            'image_monitor': AttributeDict({\n",
    "                'train_global_steps': 100,\n",
    "                'samples_per_log': 4\n",
    "            })\n",
    "        })\n",
    "        self.compile = AttributeDict({\n",
    "            'suppress_errors': True,\n",
    "            'print_specializations': True,\n",
    "            'disable': True\n",
    "        })\n",
    "        "
   ],
   "id": "2d68f3eb2bee59a7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.132094Z",
     "start_time": "2024-09-26T18:44:22.129414Z"
    }
   },
   "cell_type": "code",
   "source": "cfg = CFG()",
   "id": "df838d83b2ff325c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.181989Z",
     "start_time": "2024-09-26T18:44:22.179518Z"
    }
   },
   "cell_type": "code",
   "source": "actual_logger = logger.logger",
   "id": "a28063d858adbf10",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.231312Z",
     "start_time": "2024-09-26T18:44:22.229299Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3580c4b040f3963c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.281974Z",
     "start_time": "2024-09-26T18:44:22.279089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not actual_logger.handlers:\n",
    "    handler = logging.StreamHandler()  # Default to console output\n",
    "    handler.setLevel(logging.INFO)\n",
    "    actual_logger.addHandler(handler)"
   ],
   "id": "b22a59527b758138",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.337390Z",
     "start_time": "2024-09-26T18:44:22.334518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for handler in logger.logger.handlers:\n",
    "    print(f\"Handler: {handler}\")\n",
    "    print(f\"Handler Type: {type(handler)}\")\n",
    "    if isinstance(handler, logging.FileHandler):\n",
    "        print(f\"Logging to file: {handler.baseFilename}\")\n",
    "    elif isinstance(handler, logging.StreamHandler):\n",
    "        print(\"Logging to console/stream\")"
   ],
   "id": "a0b3d66b21c494b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handler: <StreamHandler stderr (INFO)>\n",
      "Handler Type: <class 'logging.StreamHandler'>\n",
      "Logging to console/stream\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.413441Z",
     "start_time": "2024-09-26T18:44:22.395201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Copyright (c) 2023-2024, Zexin He\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "# def parse_configs():\n",
    "#     # Define argparse arguments\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--config', type=str, default='./assets/config.yaml')\n",
    "#     args, unknown = parser.parse_known_args()\n",
    "# \n",
    "#     # Load configuration file\n",
    "#     cfg = OmegaConf.load(args.config)\n",
    "# \n",
    "#     # Override with command-line arguments\n",
    "#     cli_cfg = OmegaConf.from_cli(unknown)\n",
    "#     cfg = OmegaConf.merge(cfg, cli_cfg)\n",
    "# \n",
    "#     return cfg\n",
    "\n",
    "\n",
    "class Trainer(Runner):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        self.accelerator = Accelerator(\n",
    "            mixed_precision=self.cfg.train.mixed_precision,\n",
    "            gradient_accumulation_steps=self.cfg.train.accum_steps,\n",
    "            log_with=tuple(self.cfg.logger.trackers),\n",
    "            project_config=ProjectConfiguration(\n",
    "                logging_dir=self.cfg.logger.tracker_root,\n",
    "            ),\n",
    "            use_seedable_sampler=True,\n",
    "            kwargs_handlers=[\n",
    "                DistributedDataParallelKwargs(\n",
    "                    find_unused_parameters=self.cfg.train.find_unused_parameters,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "        set_seed(self.cfg.experiment.seed, device_specific=True)\n",
    "        with self.accelerator.main_process_first():\n",
    "            configure_logger(\n",
    "                stream_level=self.cfg.logger.stream_level,\n",
    "                log_level=self.cfg.logger.log_level,\n",
    "                file_path=os.path.join(\n",
    "                    self.cfg.logger.log_root,\n",
    "                    self.cfg.experiment.parent, self.cfg.experiment.child,\n",
    "                    f\"{self.timestamp}.log\",\n",
    "                ) if self.accelerator.is_main_process else None,\n",
    "            )\n",
    "        logger.info(self.accelerator.state, main_process_only=False, in_order=True)\n",
    "        configure_dynamo(dict(self.cfg.compile))\n",
    "\n",
    "        # attributes with defaults\n",
    "        self.model : torch.nn.Module = None\n",
    "        self.optimizer: torch.optim.Optimizer = None\n",
    "        self.scheduler: torch.optim.lr_scheduler.LRScheduler = None\n",
    "        self.train_loader: torch.utils.data.DataLoader = None\n",
    "        self.val_loader: torch.utils.data.DataLoader = None\n",
    "        self.N_max_global_steps: int = None\n",
    "        self.N_global_steps_per_epoch: int = None\n",
    "        self.global_step: int = 0\n",
    "        self.current_epoch: int = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.accelerator.init_trackers(\n",
    "            project_name=f\"{self.cfg.experiment.parent}/{self.cfg.experiment.child}\",\n",
    "        )\n",
    "        self.prepare_everything()\n",
    "        self.log_inital_info()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.accelerator.end_training()\n",
    "\n",
    "    @staticmethod\n",
    "    def control(option: str = None, synchronized: bool = False):\n",
    "        def decorator(func):\n",
    "            def wrapper(self, *args, **kwargs):\n",
    "                if option is None or hasattr(self.accelerator, option):\n",
    "                    accelerated_func = getattr(self.accelerator, option)(func) if option is not None else func\n",
    "                    result = accelerated_func(self, *args, **kwargs)\n",
    "                    if synchronized:\n",
    "                        self.accelerator.wait_for_everyone()\n",
    "                    return result\n",
    "                else:\n",
    "                    raise AttributeError(f\"Accelerator has no attribute {option}\")\n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "    @contextmanager\n",
    "    def exec_in_order(self):\n",
    "        for rank in range(self.accelerator.num_processes):\n",
    "            try:\n",
    "                if self.accelerator.process_index == rank:\n",
    "                    yield\n",
    "            finally:\n",
    "                self.accelerator.wait_for_everyone()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    @property\n",
    "    def is_distributed(self) -> bool:\n",
    "        return self.accelerator.num_processes > 1\n",
    "\n",
    "    def prepare_everything(self, is_dist_validation: bool = True):\n",
    "        # prepare with accelerator\n",
    "        if is_dist_validation:\n",
    "            self.model, self.optimizer, self.train_loader, self.val_loader = \\\n",
    "                self.accelerator.prepare(\n",
    "                    self.model, self.optimizer, self.train_loader, self.val_loader,\n",
    "                )\n",
    "        else:\n",
    "            self.model, self.optimizer, self.train_loader = \\\n",
    "                self.accelerator.prepare(\n",
    "                    self.model, self.optimizer, self.train_loader,\n",
    "                )\n",
    "        self.accelerator.register_for_checkpointing(self.scheduler)\n",
    "        # prepare stats\n",
    "        N_total_batch_size = self.cfg.train.batch_size * self.accelerator.num_processes * self.cfg.train.accum_steps\n",
    "        self.N_global_steps_per_epoch = math.ceil(len(self.train_loader) / self.cfg.train.accum_steps)\n",
    "        self.N_max_global_steps = self.N_global_steps_per_epoch * self.cfg.train.epochs\n",
    "        if self.cfg.train.debug_global_steps is not None:\n",
    "            logger.warning(f\"Overriding max global steps from {self.N_max_global_steps} to {self.cfg.train.debug_global_steps}\")\n",
    "            self.N_max_global_steps = self.cfg.train.debug_global_steps\n",
    "        logger.info(f\"======== Statistics ========\")\n",
    "        logger.info(f\"** N_max_global_steps: {self.N_max_global_steps}\")\n",
    "        logger.info(f\"** N_total_batch_size: {N_total_batch_size}\")\n",
    "        logger.info(f\"** N_epochs: {self.cfg.train.epochs}\")\n",
    "        logger.info(f\"** N_global_steps_per_epoch: {self.N_global_steps_per_epoch}\")\n",
    "        logger.debug(f\"** Prepared loader length: {len(self.train_loader)}\")\n",
    "        logger.info(f\"** Distributed validation: {is_dist_validation}\")\n",
    "        logger.info(f\"============================\")\n",
    "        logger.info(f\"======== Trainable parameters ========\")\n",
    "        logger.info(f\"** Total: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}\")\n",
    "        for sub_name, sub_module in self.accelerator.unwrap_model(self.model).named_children():\n",
    "            logger.info(f\"** {sub_name}: {sum(p.numel() for p in sub_module.parameters() if p.requires_grad)}\")\n",
    "        logger.info(f\"=====================================\")\n",
    "        self.accelerator.wait_for_everyone()\n",
    "        # load checkpoint or model\n",
    "        self.load_ckpt_or_auto_resume_(self.cfg)\n",
    "        # register hooks\n",
    "        self.register_hooks()\n",
    "\n",
    "    @abstractmethod\n",
    "    def register_hooks(self):\n",
    "        pass\n",
    "\n",
    "    def auto_resume_(self, cfg) -> bool:\n",
    "        ckpt_root = os.path.join(\n",
    "            cfg.saver.checkpoint_root,\n",
    "            cfg.experiment.parent, cfg.experiment.child,\n",
    "        )\n",
    "        if not os.path.exists(ckpt_root):\n",
    "            return False\n",
    "        ckpt_dirs = os.listdir(ckpt_root)\n",
    "        if len(ckpt_dirs) == 0:\n",
    "            return False\n",
    "        ckpt_dirs.sort()\n",
    "        latest_ckpt = ckpt_dirs[-1]\n",
    "        latest_ckpt_dir = os.path.join(ckpt_root, latest_ckpt)\n",
    "        logger.info(f\"======== Auto-resume from {latest_ckpt_dir} ========\")\n",
    "        self.accelerator.load_state(latest_ckpt_dir)\n",
    "        self.global_step = int(latest_ckpt)\n",
    "        self.current_epoch = self.global_step // self.N_global_steps_per_epoch\n",
    "        return True\n",
    "\n",
    "    def load_model_(self, cfg):\n",
    "        logger.info(f\"======== Loading model from {cfg.saver.load_model} ========\")\n",
    "        safetensors.torch.load_model(\n",
    "            self.accelerator.unwrap_model(self.model),\n",
    "            cfg.saver.load_model,\n",
    "            strict=True,\n",
    "        )\n",
    "        logger.info(f\"======== Model loaded ========\")\n",
    "\n",
    "    @control(synchronized=True)\n",
    "    def load_ckpt_or_auto_resume_(self, cfg):\n",
    "        # auto resume has higher priority, load model from path if auto resume is not available\n",
    "        # cfg.saver.auto_resume and cfg.saver.load_model\n",
    "        if cfg.saver.auto_resume:\n",
    "            successful_resume = self.auto_resume_(cfg)\n",
    "            if successful_resume:\n",
    "                return\n",
    "        if cfg.saver.load_model:\n",
    "            successful_load = self.load_model_(cfg)\n",
    "            if successful_load:\n",
    "                return\n",
    "        logger.debug(f\"======== No checkpoint or model is loaded ========\")\n",
    "\n",
    "    @control('on_main_process', synchronized=True)\n",
    "    def save_checkpoint(self):\n",
    "        ckpt_dir = os.path.join(\n",
    "            self.cfg.saver.checkpoint_root,\n",
    "            self.cfg.experiment.parent, self.cfg.experiment.child,\n",
    "            f\"{self.global_step:06d}\",\n",
    "        )\n",
    "        self.accelerator.save_state(output_dir=ckpt_dir, safe_serialization=True)\n",
    "        logger.info(f\"======== Saved checkpoint at global step {self.global_step} ========\")\n",
    "        # manage stratified checkpoints\n",
    "        ckpt_dirs = os.listdir(os.path.dirname(ckpt_dir))\n",
    "        ckpt_dirs.sort()\n",
    "        max_ckpt = int(ckpt_dirs[-1])\n",
    "        ckpt_base = int(self.cfg.saver.checkpoint_keep_level)\n",
    "        ckpt_period = self.cfg.saver.checkpoint_global_steps\n",
    "        logger.debug(f\"Checkpoint base: {ckpt_base}\")\n",
    "        logger.debug(f\"Checkpoint period: {ckpt_period}\")\n",
    "        print(ckpt_base, ckpt_period, max_ckpt)\n",
    "         \n",
    "        e = 0\n",
    "        if max_ckpt // ckpt_period == 0:\n",
    "            e = 1e-5\n",
    "        cur_order = ckpt_base ** math.floor(math.log((max_ckpt // ckpt_period) + e, ckpt_base))\n",
    "        cur_idx = 0\n",
    "        while cur_order > 0:\n",
    "            cur_digit = max_ckpt // ckpt_period // cur_order % ckpt_base\n",
    "            while cur_idx < len(ckpt_dirs) and int(ckpt_dirs[cur_idx]) // ckpt_period // cur_order % ckpt_base < cur_digit:\n",
    "                if int(ckpt_dirs[cur_idx]) // ckpt_period % cur_order != 0:\n",
    "                    shutil.rmtree(os.path.join(os.path.dirname(ckpt_dir), ckpt_dirs[cur_idx]))\n",
    "                    logger.info(f\"Removed checkpoint {ckpt_dirs[cur_idx]}\")\n",
    "                cur_idx += 1\n",
    "            cur_order //= ckpt_base\n",
    "\n",
    "    @property\n",
    "    def global_step_in_epoch(self):\n",
    "        print(self.global_step, self.N_global_steps_per_epoch)\n",
    "        if(self.N_global_steps_per_epoch == 0):\n",
    "            self.N_global_steps_per_epoch = 1\n",
    "        #input(\"remember to change global_step_in_epoch\")\n",
    "        return self.global_step % self.N_global_steps_per_epoch\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_model(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_optimizer(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_scheduler(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_loss_fn(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_str_progress(epoch: int = None, step: int = None):\n",
    "        if epoch is not None:\n",
    "            log_type = 'epoch'\n",
    "            log_progress = epoch\n",
    "        elif step is not None:\n",
    "            log_type = 'step'\n",
    "            log_progress = step\n",
    "        else:\n",
    "            raise ValueError('Either epoch or step must be provided')\n",
    "        return log_type, log_progress\n",
    "\n",
    "    @control('on_main_process')\n",
    "    def log_scalar_kwargs(self, epoch: int = None, step: int = None, split: str = None, **scalar_kwargs):\n",
    "        log_type, log_progress = self._get_str_progress(epoch, step)\n",
    "        split = f'/{split}' if split else ''\n",
    "        for key, value in scalar_kwargs.items():\n",
    "            self.accelerator.log({f'{key}{split}/{log_type}': value}, log_progress)\n",
    "\n",
    "    @control('on_main_process')\n",
    "    def log_images(self, values: dict, step: int | None = None, log_kwargs: dict | None = {}):\n",
    "        for tracker in self.accelerator.trackers:\n",
    "            if hasattr(tracker, 'log_images'):\n",
    "                tracker.log_images(values, step=step, **log_kwargs.get(tracker.name, {}))\n",
    "\n",
    "    @control('on_main_process')\n",
    "    def log_optimizer(self, epoch: int = None, step: int = None, attrs: list[str] = [], group_ids: list[int] = []):\n",
    "        log_type, log_progress = self._get_str_progress(epoch, step)\n",
    "        assert self.optimizer is not None, 'Optimizer is not initialized'\n",
    "        if not attrs:\n",
    "            logger.warning('No optimizer attributes are provided, nothing will be logged')\n",
    "        if not group_ids:\n",
    "            logger.warning('No optimizer group ids are provided, nothing will be logged')\n",
    "        for attr in attrs:\n",
    "            assert attr in ['lr', 'momentum', 'weight_decay'], f'Invalid optimizer attribute {attr}'\n",
    "            for group_id in group_ids:\n",
    "                self.accelerator.log({f'opt/{attr}/{group_id}': self.optimizer.param_groups[group_id][attr]}, log_progress)\n",
    "\n",
    "    @control('on_main_process')\n",
    "    def log_inital_info(self):\n",
    "        assert self.model is not None, 'Model is not initialized'\n",
    "        assert self.optimizer is not None, 'Optimizer is not initialized'\n",
    "        assert self.scheduler is not None, 'Scheduler is not initialized'\n",
    "        self.accelerator.log({'Config': \"```\\n\" + OmegaConf.to_yaml(self.cfg) + \"\\n```\"})\n",
    "        self.accelerator.log({'Model': \"```\\n\" + str(self.model) + \"\\n```\"})\n",
    "        self.accelerator.log({'Optimizer': \"```\\n\" + str(self.optimizer) + \"\\n```\"})\n",
    "        self.accelerator.log({'Scheduler': \"```\\n\" + str(self.scheduler) + \"\\n```\"})\n",
    "\n",
    "    def run(self):\n",
    "        self.train()\n"
   ],
   "id": "544ede987d8cbef2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.451876Z",
     "start_time": "2024-09-26T18:44:22.449532Z"
    }
   },
   "cell_type": "code",
   "source": "#5 ** math.floor(math.log(60 // 1000, 5))",
   "id": "b1d8de03a49c5478",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.501559Z",
     "start_time": "2024-09-26T18:44:22.499667Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2aabc6f911c66c4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.562292Z",
     "start_time": "2024-09-26T18:44:22.558034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "e = 0\n",
    "if 60 // 1000 == 0:\n",
    "    e = 1e-5\n",
    "5 ** math.floor(math.log((60 // 1000) + e, 5))"
   ],
   "id": "23c7627413b74301",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.56e-06"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.676487Z",
     "start_time": "2024-09-26T18:44:22.657740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# @REGISTRY_RUNNERS.register('train.lrm')\n",
    "class LRMTrainer(Trainer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = self._build_model(self.cfg)\n",
    "        self.optimizer = self._build_optimizer(self.model, self.cfg)\n",
    "        self.train_loader, self.val_loader = self._build_dataloader(self.cfg)\n",
    "        self.scheduler = self._build_scheduler(self.optimizer, self.cfg)\n",
    "        self.pixel_loss_fn, self.perceptual_loss_fn, self.tv_loss_fn = self._build_loss_fn(self.cfg)\n",
    "\n",
    "    def _build_model(self, cfg):\n",
    "        assert cfg.experiment.type == 'lrm', \\\n",
    "            f\"Config type {cfg.experiment.type} does not match with runner {self.__class__.__name__}\"\n",
    "        from openlrm.models import ModelLRM\n",
    "        model = ModelLRM(**cfg.model)\n",
    "        return model\n",
    "\n",
    "    def _build_optimizer(self, model: nn.Module, cfg):\n",
    "        decay_params, no_decay_params = [], []\n",
    "\n",
    "        # add all bias and LayerNorm params to no_decay_params\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.LayerNorm):\n",
    "                no_decay_params.extend([p for p in module.parameters()])\n",
    "            elif hasattr(module, 'bias') and module.bias is not None:\n",
    "                no_decay_params.append(module.bias)\n",
    "\n",
    "        # add remaining parameters to decay_params\n",
    "        _no_decay_ids = set(map(id, no_decay_params))\n",
    "        decay_params = [p for p in model.parameters() if id(p) not in _no_decay_ids]\n",
    "\n",
    "        # filter out parameters with no grad\n",
    "        decay_params = list(filter(lambda p: p.requires_grad, decay_params))\n",
    "        no_decay_params = list(filter(lambda p: p.requires_grad, no_decay_params))\n",
    "\n",
    "        # monitor this to make sure we don't miss any parameters\n",
    "        logger.info(\"======== Weight Decay Parameters ========\")\n",
    "        logger.info(f\"Total: {len(decay_params)}\")\n",
    "        logger.info(\"======== No Weight Decay Parameters ========\")\n",
    "        logger.info(f\"Total: {len(no_decay_params)}\")\n",
    "\n",
    "        # Optimizer\n",
    "        opt_groups = [\n",
    "            {'params': decay_params, 'weight_decay': cfg.train.optim.weight_decay},\n",
    "            {'params': no_decay_params, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            opt_groups,\n",
    "            lr=cfg.train.optim.lr,\n",
    "            betas=(cfg.train.optim.beta1, cfg.train.optim.beta2),\n",
    "        )\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def _build_scheduler(self, optimizer, cfg):\n",
    "        local_batches_per_epoch = math.floor(len(self.train_loader) / self.accelerator.num_processes)\n",
    "        total_global_batches = cfg.train.epochs * math.ceil(local_batches_per_epoch / self.cfg.train.accum_steps)\n",
    "        effective_warmup_iters = cfg.train.scheduler.warmup_real_iters\n",
    "        logger.debug(f\"======== Scheduler effective max iters: {total_global_batches} ========\")\n",
    "        logger.debug(f\"======== Scheduler effective warmup iters: {effective_warmup_iters} ========\")\n",
    "        if cfg.train.scheduler.type == 'cosine':\n",
    "            from openlrm.utils.scheduler import CosineWarmupScheduler\n",
    "            scheduler = CosineWarmupScheduler(\n",
    "                optimizer=optimizer,\n",
    "                warmup_iters=effective_warmup_iters,\n",
    "                max_iters=total_global_batches,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Scheduler type {cfg.train.scheduler.type} not implemented\")\n",
    "        return scheduler\n",
    "\n",
    "    def _build_dataloader(self, cfg):\n",
    "        # dataset class\n",
    "        from openlrm.datasets import MixerDataset\n",
    "\n",
    "        # build dataset\n",
    "        train_dataset = MixerDataset(\n",
    "            split=\"train\",\n",
    "            subsets=cfg.dataset.subsets,\n",
    "            sample_side_views=cfg.dataset.sample_side_views,\n",
    "            render_image_res_low=cfg.dataset.render_image.low,\n",
    "            render_image_res_high=cfg.dataset.render_image.high,\n",
    "            render_region_size=cfg.dataset.render_image.region,\n",
    "            source_image_res=cfg.dataset.source_image_res,\n",
    "            normalize_camera=cfg.dataset.normalize_camera,\n",
    "            normed_dist_to_center=cfg.dataset.normed_dist_to_center,\n",
    "        )\n",
    "        val_dataset = MixerDataset(\n",
    "            split=\"val\",\n",
    "            subsets=cfg.dataset.subsets,\n",
    "            sample_side_views=cfg.dataset.sample_side_views,\n",
    "            render_image_res_low=cfg.dataset.render_image.low,\n",
    "            render_image_res_high=cfg.dataset.render_image.high,\n",
    "            render_region_size=cfg.dataset.render_image.region,\n",
    "            source_image_res=cfg.dataset.source_image_res,\n",
    "            normalize_camera=cfg.dataset.normalize_camera,\n",
    "            normed_dist_to_center=cfg.dataset.normed_dist_to_center,\n",
    "        )\n",
    "\n",
    "        # build data loader\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=cfg.train.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=cfg.dataset.num_train_workers,\n",
    "            pin_memory=cfg.dataset.pin_mem,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    " \n",
    "       \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=cfg.val.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=cfg.dataset.num_val_workers,\n",
    "            pin_memory=cfg.dataset.pin_mem,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def _build_loss_fn(self, cfg):\n",
    "        from openlrm.losses import PixelLoss, LPIPSLoss, TVLoss\n",
    "        pixel_loss_fn = PixelLoss()\n",
    "        with self.accelerator.main_process_first():\n",
    "            perceptual_loss_fn = LPIPSLoss(device=self.device, prefech=True)\n",
    "        tv_loss_fn = TVLoss()\n",
    "        return pixel_loss_fn, perceptual_loss_fn, tv_loss_fn\n",
    "\n",
    "    def register_hooks(self):\n",
    "        pass\n",
    "\n",
    "    def forward_loss_local_step(self, data):\n",
    "\n",
    "        source_camera = data['source_camera']\n",
    "        render_camera = data['render_camera']\n",
    "        source_image = data['source_image']\n",
    "        render_image = data['render_image']\n",
    "        render_anchors = data['render_anchors']\n",
    "        render_full_resolutions = data['render_full_resolutions']\n",
    "        render_bg_colors = data['render_bg_colors']\n",
    "\n",
    "    \n",
    "        N, M, C, H, W = render_image.shape\n",
    "        \n",
    "        # forward\n",
    "        # for params in self.model.parameters():\n",
    "        #     print(params.device)\n",
    "        outputs = self.model(\n",
    "            image=source_image,\n",
    "            source_camera=source_camera,\n",
    "            render_cameras=render_camera,\n",
    "            render_anchors=render_anchors,\n",
    "            render_resolutions=render_full_resolutions,\n",
    "            render_bg_colors=render_bg_colors,\n",
    "            render_region_size=self.cfg.dataset.render_image.region,\n",
    "        )\n",
    "\n",
    "        # loss calculation\n",
    "        loss = 0.\n",
    "        loss_pixel = None\n",
    "        loss_perceptual = None\n",
    "        loss_tv = None\n",
    "\n",
    "        if self.cfg.train.loss.pixel_weight > 0.:\n",
    "            loss_pixel = self.pixel_loss_fn(outputs['images_rgb'], render_image)\n",
    "            loss += loss_pixel * self.cfg.train.loss.pixel_weight\n",
    "        if self.cfg.train.loss.perceptual_weight > 0.:\n",
    "            loss_perceptual = self.perceptual_loss_fn(outputs['images_rgb'], render_image)\n",
    "            loss += loss_perceptual * self.cfg.train.loss.perceptual_weight\n",
    "        if self.cfg.train.loss.tv_weight > 0.: \n",
    "            loss_tv = self.tv_loss_fn(outputs['planes'])\n",
    "            loss += loss_tv * self.cfg.train.loss.tv_weight\n",
    "\n",
    "        return outputs, loss, loss_pixel, loss_perceptual, loss_tv\n",
    "    \n",
    "    \n",
    "\n",
    "    def train_epoch(self, pbar: tqdm, loader: torch.utils.data.DataLoader, profiler: torch.profiler.profile):\n",
    "        self.model.train()\n",
    "\n",
    "        local_step_losses = []\n",
    "        global_step_losses = []\n",
    "\n",
    "        logger.debug(f\"======== Starting epoch {self.current_epoch} ========\")\n",
    "        \n",
    "        \n",
    "        for data in loader:\n",
    "            \n",
    "            logger.debug(f\"======== Starting global step {self.global_step} ========\")\n",
    "            with self.accelerator.accumulate(self.model):\n",
    "\n",
    "                # forward to loss\n",
    "                outs, loss, loss_pixel, loss_perceptual, loss_tv = self.forward_loss_local_step(data)\n",
    "                \n",
    "                # backward\n",
    "                self.accelerator.backward(loss)\n",
    "                if self.accelerator.sync_gradients and self.cfg.train.optim.clip_grad_norm > 0.:\n",
    "                    self.accelerator.clip_grad_norm_(self.model.parameters(), self.cfg.train.optim.clip_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # track local losses\n",
    "                local_step_losses.append(torch.stack([\n",
    "                    _loss.detach() if _loss is not None else torch.tensor(float('nan'), device=self.device)\n",
    "                    for _loss in [loss, loss_pixel, loss_perceptual, loss_tv]\n",
    "                ]))\n",
    "\n",
    "            # track global step\n",
    "            if self.accelerator.sync_gradients:\n",
    "                profiler.step()\n",
    "                self.scheduler.step()\n",
    "                logger.debug(f\"======== Scheduler step ========\")\n",
    "                self.global_step += 1\n",
    "                global_step_loss = self.accelerator.gather(torch.stack(local_step_losses)).mean(dim=0).cpu()\n",
    "                loss, loss_pixel, loss_perceptual, loss_tv = global_step_loss.unbind()\n",
    "                loss_kwargs = {\n",
    "                    'loss': loss.item(),\n",
    "                    'loss_pixel': loss_pixel.item(),\n",
    "                    'loss_perceptual': loss_perceptual.item(),\n",
    "                    'loss_tv': loss_tv.item(),\n",
    "                }\n",
    "                self.log_scalar_kwargs(\n",
    "                    step=self.global_step, split='train',\n",
    "                    **loss_kwargs\n",
    "                )\n",
    "                self.log_optimizer(step=self.global_step, attrs=['lr'], group_ids=[0, 1])\n",
    "                local_step_losses = []\n",
    "                global_step_losses.append(global_step_loss)\n",
    "\n",
    "                # manage display\n",
    "                pbar.update(1)\n",
    "                description = {\n",
    "                    **loss_kwargs,\n",
    "                    'lr': self.optimizer.param_groups[0]['lr'],\n",
    "                }\n",
    "                description = '[TRAIN STEP]' + \\\n",
    "                    ', '.join(f'{k}={tqdm.format_num(v)}' for k, v in description.items() if not math.isnan(v))\n",
    "                pbar.set_description(description)\n",
    "\n",
    "                # periodic actions\n",
    "                if self.global_step % self.cfg.saver.checkpoint_global_steps == 0:\n",
    "                    self.save_checkpoint()\n",
    "                if self.global_step % self.cfg.val.global_step_period == 0:\n",
    "                    self.evaluate()\n",
    "                    self.model.train()\n",
    "                if self.global_step % self.cfg.logger.image_monitor.train_global_steps == 0:\n",
    "                    self.log_image_monitor(\n",
    "                        step=self.global_step, split='train',\n",
    "                        renders=outs['images_rgb'].detach()[:self.cfg.logger.image_monitor.samples_per_log].cpu(),\n",
    "                        gts=data['render_image'][:self.cfg.logger.image_monitor.samples_per_log].cpu(),\n",
    "                    )\n",
    "\n",
    "                # progress control\n",
    "                if self.global_step >= self.N_max_global_steps:\n",
    "                    self.accelerator.set_trigger()\n",
    "                    break\n",
    "\n",
    "        # track epoch\n",
    "        self.current_epoch += 1\n",
    "        epoch_losses = torch.stack(global_step_losses).mean(dim=0)\n",
    "        epoch_loss, epoch_loss_pixel, epoch_loss_perceptual, epoch_loss_tv = epoch_losses.unbind()\n",
    "        epoch_loss_dict = {\n",
    "            'loss': epoch_loss.item(),\n",
    "            'loss_pixel': epoch_loss_pixel.item(),\n",
    "            'loss_perceptual': epoch_loss_perceptual.item(),\n",
    "            'loss_tv': epoch_loss_tv.item(),\n",
    "        }\n",
    "        self.log_scalar_kwargs(\n",
    "            epoch=self.current_epoch, split='train',\n",
    "            **epoch_loss_dict,\n",
    "        )\n",
    "        logger.info(\n",
    "            f'[TRAIN EPOCH] {self.current_epoch}/{self.cfg.train.epochs}: ' + \\\n",
    "                ', '.join(f'{k}={tqdm.format_num(v)}' for k, v in epoch_loss_dict.items() if not math.isnan(v))\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        starting_local_step_in_epoch = self.global_step_in_epoch * self.cfg.train.accum_steps\n",
    "        skipped_loader = self.accelerator.skip_first_batches(self.train_loader, starting_local_step_in_epoch)\n",
    "        logger.info(f\"======== Skipped {starting_local_step_in_epoch} local batches ========\")\n",
    "\n",
    "        with tqdm(\n",
    "            range(0, self.N_max_global_steps),\n",
    "            initial=self.global_step,\n",
    "            disable=(not self.accelerator.is_main_process),\n",
    "        ) as pbar:\n",
    "\n",
    "            profiler = torch.profiler.profile(\n",
    "                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "                schedule=torch.profiler.schedule(\n",
    "                    wait=10, warmup=10, active=100,\n",
    "                ),\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(os.path.join(\n",
    "                    self.cfg.logger.tracker_root,\n",
    "                    self.cfg.experiment.parent, self.cfg.experiment.child,\n",
    "                )),\n",
    "                record_shapes=True,\n",
    "                profile_memory=True,\n",
    "                with_stack=True,\n",
    "            ) if self.cfg.logger.enable_profiler else DummyProfiler()\n",
    "            \n",
    "            with profiler:\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                for _ in range(self.current_epoch, self.cfg.train.epochs):\n",
    "\n",
    "                    loader = skipped_loader or self.train_loader\n",
    "                    #loader = self.train_loader\n",
    "                    #input(\"remember to get rid of line above\")\n",
    "                    skipped_loader = None\n",
    "                    self.train_epoch(pbar=pbar, loader=loader, profiler=profiler)\n",
    "                    if self.accelerator.check_trigger():\n",
    "                        break\n",
    "\n",
    "            logger.info(f\"======== Training finished at global step {self.global_step} ========\")\n",
    "\n",
    "            # final checkpoint and evaluation\n",
    "            self.save_checkpoint()\n",
    "            self.evaluate()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.compiler.disable\n",
    "    def evaluate(self, epoch: int = None):\n",
    "        self.model.eval()\n",
    "\n",
    "        max_val_batches = self.cfg.val.debug_batches or len(self.val_loader)\n",
    "        running_losses = []\n",
    "        sample_data, sample_outs = None, None\n",
    "\n",
    "        for data in tqdm(self.val_loader, disable=(not self.accelerator.is_main_process), total=max_val_batches):\n",
    "\n",
    "            if len(running_losses) >= max_val_batches:\n",
    "                logger.info(f\"======== Early stop validation at {len(running_losses)} batches ========\")\n",
    "                break\n",
    "\n",
    "            outs, loss, loss_pixel, loss_perceptual, loss_tv = self.forward_loss_local_step(data)\n",
    "            sample_data, sample_outs = data, outs\n",
    "\n",
    "            running_losses.append(torch.stack([\n",
    "                _loss if _loss is not None else torch.tensor(float('nan'), device=self.device)\n",
    "                for _loss in [loss, loss_pixel, loss_perceptual, loss_tv]\n",
    "            ]))\n",
    "\n",
    "        total_losses = self.accelerator.gather(torch.stack(running_losses)).mean(dim=0).cpu()\n",
    "        total_loss, total_loss_pixel, total_loss_perceptual, total_loss_tv = total_losses.unbind()\n",
    "        total_loss_dict = {\n",
    "            'loss': total_loss.item(),\n",
    "            'loss_pixel': total_loss_pixel.item(),\n",
    "            'loss_perceptual': total_loss_perceptual.item(),\n",
    "            'loss_tv': total_loss_tv.item(),\n",
    "        }\n",
    "\n",
    "        if epoch is not None:\n",
    "            self.log_scalar_kwargs(\n",
    "                epoch=epoch, split='val',\n",
    "                **total_loss_dict,\n",
    "            )\n",
    "            logger.info(\n",
    "                f'[VAL EPOCH] {epoch}/{self.cfg.train.epochs}: ' + \\\n",
    "                    ', '.join(f'{k}={tqdm.format_num(v)}' for k, v in total_loss_dict.items() if not math.isnan(v))\n",
    "            )\n",
    "            self.log_image_monitor(\n",
    "                epoch=epoch, split='val',\n",
    "                renders=sample_outs['images_rgb'][:self.cfg.logger.image_monitor.samples_per_log].cpu(),\n",
    "                gts=sample_data['render_image'][:self.cfg.logger.image_monitor.samples_per_log].cpu(),\n",
    "            )\n",
    "        else:\n",
    "            self.log_scalar_kwargs(\n",
    "                step=self.global_step, split='val',\n",
    "                **total_loss_dict,\n",
    "            )\n",
    "            logger.info(\n",
    "                f'[VAL STEP] {self.global_step}/{self.N_max_global_steps}: ' + \\\n",
    "                    ', '.join(f'{k}={tqdm.format_num(v)}' for k, v in total_loss_dict.items() if not math.isnan(v))\n",
    "            )\n",
    "            self.log_image_monitor(\n",
    "                step=self.global_step, split='val',\n",
    "                renders=sample_outs['images_rgb'][:self.cfg.logger.image_monitor.samples_per_log].cpu(),\n",
    "                gts=sample_data['render_image'][:self.cfg.logger.image_monitor.samples_per_log].cpu(),\n",
    "            )\n",
    "\n",
    "    @Trainer.control('on_main_process')\n",
    "    def log_image_monitor(\n",
    "        self, epoch: int = None, step: int = None, split: str = None,\n",
    "        renders: torch.Tensor = None, gts: torch.Tensor = None,\n",
    "        ):\n",
    "        M = renders.shape[1]\n",
    "        merged = torch.stack([renders, gts], dim=1)[0].view(-1, *renders.shape[2:])\n",
    "        renders, gts = renders.view(-1, *renders.shape[2:]), gts.view(-1, *gts.shape[2:])\n",
    "        renders, gts, merged = make_grid(renders, nrow=M), make_grid(gts, nrow=M), make_grid(merged, nrow=M)\n",
    "        log_type, log_progress = self._get_str_progress(epoch, step)\n",
    "        split = f'/{split}' if split else ''\n",
    "        self.log_images({\n",
    "            f'Images_split{split}/rendered': renders.unsqueeze(0),\n",
    "            f'Images_split{split}/gt': gts.unsqueeze(0),\n",
    "            f'Images_merged{split}': merged.unsqueeze(0),\n",
    "        }, log_progress)\n"
   ],
   "id": "75548dcc5ac776dc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.719710Z",
     "start_time": "2024-09-26T18:44:22.715786Z"
    }
   },
   "cell_type": "code",
   "source": "cfg.experiment",
   "id": "ae80b436d901e15e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'lrm',\n",
       " 'seed': 42,\n",
       " 'parent': 'lrm-objaverse',\n",
       " 'child': 'small-dummyrun'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:22.771974Z",
     "start_time": "2024-09-26T18:44:22.769074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# {'experiment': {'type': 'lrm', 'seed': 42, 'parent': 'lrm-objaverse', 'child': 'small-dummyrun'}, 'model': {'camera_embed_dim': 1024, 'rendering_samples_per_ray': 96, 'transformer_dim': 512, 'transform\n",
    "# er_layers': 12, 'transformer_heads': 8, 'triplane_low_res': 32, 'triplane_high_res': 64, 'triplane_dim': 32, 'encoder_type': 'dinov2', 'encoder_model_name': 'dinov2_vits14_reg', 'encoder_feat_dim': 384\n",
    "# , 'encoder_freeze': False}, 'dataset': {'subsets': [{'name': 'objaverse', 'root_dirs': ['<REPLACE_WITH_RENDERING_ROOT>'], 'meta_path': {'train': '<TRAIN_UIDS_IN_JSON>', 'val': '<VAL_UIDS_IN_JSON>'}, 's\n",
    "# ample_rate': 1.0}], 'sample_side_views': 3, 'source_image_res': 224, 'render_image': {'low': 64, 'high': 192, 'region': 64}, 'normalize_camera': True, 'normed_dist_to_center': 'auto', 'num_train_worker\n",
    "# s': 4, 'num_val_workers': 2, 'pin_mem': True}, 'train': {'mixed_precision': 'bf16', 'find_unused_parameters': False, 'loss': {'pixel_weight': 1.0, 'perceptual_weight': 1.0, 'tv_weight': 0.0005}, 'optim\n",
    "# ': {'lr': 0.0004, 'weight_decay': 0.05, 'beta1': 0.9, 'beta2': 0.95, 'clip_grad_norm': 1.0}, 'scheduler': {'type': 'cosine', 'warmup_real_iters': 3000}, 'batch_size': 16, 'accum_steps': 1, 'epochs': 60\n",
    "# , 'debug_global_steps': None, 'lrm': None}, 'val': {'batch_size': 4, 'global_step_period': 1000, 'debug_batches': None}, 'saver': {'auto_resume': True, 'load_model': None, 'checkpoint_root': './exps/ch\n",
    "# eckpoints', 'checkpoint_global_steps': 1000, 'checkpoint_keep_level': 5}, 'logger': {'stream_level': 'WARNING', 'log_level': 'INFO', 'log_root': './exps/logs', 'tracker_root': './exps/trackers', 'enabl\n",
    "# e_profiler': False, 'trackers': ['tensorboard'], 'image_monitor': {'train_global_steps': 100, 'samples_per_log': 4}}, 'compile': {'suppress_errors': True, 'print_specializations': True, 'disable': True}}"
   ],
   "id": "604c12bd550bc01c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:29.421858Z",
     "start_time": "2024-09-26T18:44:22.825199Z"
    }
   },
   "cell_type": "code",
   "source": "lrm_trainer = LRMTrainer()",
   "id": "daa0bddd56ae3389",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-26 14:44:23,108] openlrm.utils.compile: [WARNING] Overriding torch._dynamo.config.suppress_errors from False to True\n",
      "[2024-09-26 14:44:23,110] openlrm.utils.compile: [WARNING] Overriding torch._dynamo.config.print_specializations from False to True\n",
      "[2024-09-26 14:44:23,110] openlrm.utils.compile: [WARNING] Overriding torch._dynamo.config.disable from False to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinsonunix/miniconda3/envs/openlrm/lib/python3.11/site-packages/accelerate/accelerator.py:412: UserWarning: `log_with=('tensorboard',)` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "/home/robinsonunix/miniconda3/envs/openlrm/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/OpenLRM/openlrm/models/encoders/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/OpenLRM/openlrm/models/encoders/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/OpenLRM/openlrm/models/encoders/dinov2/layers/block.py:46: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:57.464548Z",
     "start_time": "2024-09-26T18:44:29.765850Z"
    }
   },
   "cell_type": "code",
   "source": "lrm_trainer.prepare_everything()",
   "id": "cff22f4f94108688",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:57.942301Z",
     "start_time": "2024-09-26T18:44:57.484094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Man standing with gray shirt and brown pants, standing in a rocky surface\", return_tensors=\"pt\")"
   ],
   "id": "620cefe0436de1d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinsonunix/miniconda3/envs/openlrm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:57.976209Z",
     "start_time": "2024-09-26T18:44:57.973345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.compile\n",
    "def forward_planes(image, camera):\n",
    "    # image: [N, C_img, H_img, W_img]\n",
    "    # camera: [N, D_cam_raw]\n",
    "    \n",
    "    N = image.shape[0]\n",
    "    \n",
    "    # encode image\n",
    "    image_feats = lrm_trainer.model.encoder(image)\n",
    "    assert image_feats.shape[-1] == lrm_trainer.model.encoder_feat_dim, \\\n",
    "        f\"Feature dimension mismatch: {image_feats.shape[-1]} vs {lrm_trainer.model.encoder_feat_dim}\"\n",
    "\n",
    "    # embed camera\n",
    "    camera_embeddings = lrm_trainer.model.camera_embedder(camera)\n",
    "    assert camera_embeddings.shape[-1] == lrm_trainer.model.camera_embed_dim, \\\n",
    "        f\"Feature dimension mismatch: {camera_embeddings.shape[-1]} vs {lrm_trainer.model.camera_embed_dim}\"\n",
    "    \n",
    "    text_feats = bert(**inputs).last_hidden_state\n",
    "    # transformer generating planes\n",
    "    feats = image_feats/2 + text_feats[:,0,:]/2\n",
    "    \n",
    "\n",
    "    tokens = lrm_trainer.model.forward_transformer(feats, camera_embeddings)\n",
    "    planes = lrm_trainer.model.reshape_upsample(tokens)\n",
    "    assert planes.shape[0] == N, \"Batch size mismatch for planes\"\n",
    "    assert planes.shape[1] == 3, \"Planes should have 3 channels\"\n",
    "\n",
    "    return planes"
   ],
   "id": "114bcbcbd47e9355",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:44:58.005672Z",
     "start_time": "2024-09-26T18:44:58.003299Z"
    }
   },
   "cell_type": "code",
   "source": "lrm_trainer.model.forward_planes = forward_planes",
   "id": "2110b6a04e1e273d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:45:07.257188Z",
     "start_time": "2024-09-26T18:44:58.056428Z"
    }
   },
   "cell_type": "code",
   "source": "lrm_trainer.train()",
   "id": "6d7920bf8a94db14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1000 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:01<?, ?it/s]\u001B[A\n",
      "100%|██████████| 60/60 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (384) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m lrm_trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "Cell \u001B[0;32mIn[11], line 324\u001B[0m, in \u001B[0;36mLRMTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;66;03m# final checkpoint and evaluation\u001B[39;00m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_checkpoint()\n\u001B[0;32m--> 324\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate()\n",
      "File \u001B[0;32m~/miniconda3/envs/openlrm/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/openlrm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "Cell \u001B[0;32mIn[11], line 341\u001B[0m, in \u001B[0;36mLRMTrainer.evaluate\u001B[0;34m(self, epoch)\u001B[0m\n\u001B[1;32m    338\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m======== Early stop validation at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(running_losses)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m batches ========\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m outs, loss, loss_pixel, loss_perceptual, loss_tv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_loss_local_step(data)\n\u001B[1;32m    342\u001B[0m sample_data, sample_outs \u001B[38;5;241m=\u001B[39m data, outs\n\u001B[1;32m    344\u001B[0m running_losses\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mstack([\n\u001B[1;32m    345\u001B[0m     _loss \u001B[38;5;28;01mif\u001B[39;00m _loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnan\u001B[39m\u001B[38;5;124m'\u001B[39m), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _loss \u001B[38;5;129;01min\u001B[39;00m [loss, loss_pixel, loss_perceptual, loss_tv]\n\u001B[1;32m    347\u001B[0m ]))\n",
      "Cell \u001B[0;32mIn[11], line 152\u001B[0m, in \u001B[0;36mLRMTrainer.forward_loss_local_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    147\u001B[0m N, M, C, H, W \u001B[38;5;241m=\u001B[39m render_image\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    149\u001B[0m \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;66;03m# for params in self.model.parameters():\u001B[39;00m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;66;03m#     print(params.device)\u001B[39;00m\n\u001B[0;32m--> 152\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m    153\u001B[0m     image\u001B[38;5;241m=\u001B[39msource_image,\n\u001B[1;32m    154\u001B[0m     source_camera\u001B[38;5;241m=\u001B[39msource_camera,\n\u001B[1;32m    155\u001B[0m     render_cameras\u001B[38;5;241m=\u001B[39mrender_camera,\n\u001B[1;32m    156\u001B[0m     render_anchors\u001B[38;5;241m=\u001B[39mrender_anchors,\n\u001B[1;32m    157\u001B[0m     render_resolutions\u001B[38;5;241m=\u001B[39mrender_full_resolutions,\n\u001B[1;32m    158\u001B[0m     render_bg_colors\u001B[38;5;241m=\u001B[39mrender_bg_colors,\n\u001B[1;32m    159\u001B[0m     render_region_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mrender_image\u001B[38;5;241m.\u001B[39mregion,\n\u001B[1;32m    160\u001B[0m )\n\u001B[1;32m    162\u001B[0m \u001B[38;5;66;03m# loss calculation\u001B[39;00m\n\u001B[1;32m    163\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/openlrm/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/openlrm/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/mnt/c/Users/Robinson/OneDrive/Desktop/Classes_Fall_2024/CAP6411/Project/testing/pythonProject/OpenLRM/openlrm/models/modeling_lrm.py:143\u001B[0m, in \u001B[0;36mModelLRM.forward\u001B[0;34m(self, image, source_camera, render_cameras, render_anchors, render_resolutions, render_bg_colors, render_region_size)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m image\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m render_bg_colors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatch size mismatch for image and render_bg_colors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    141\u001B[0m N, M \u001B[38;5;241m=\u001B[39m render_cameras\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m--> 143\u001B[0m planes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_planes(image, source_camera)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;66;03m# render target views\u001B[39;00m\n\u001B[1;32m    146\u001B[0m render_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msynthesizer(planes, render_cameras, render_anchors, render_resolutions, render_bg_colors, render_region_size)\n",
      "File \u001B[0;32m~/miniconda3/envs/openlrm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "Cell \u001B[0;32mIn[17], line 20\u001B[0m, in \u001B[0;36mforward_planes\u001B[0;34m(image, camera)\u001B[0m\n\u001B[1;32m     18\u001B[0m text_feats \u001B[38;5;241m=\u001B[39m bert(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# transformer generating planes\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m feats \u001B[38;5;241m=\u001B[39m image_feats\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m text_feats[:,\u001B[38;5;241m0\u001B[39m,:]\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     23\u001B[0m tokens \u001B[38;5;241m=\u001B[39m lrm_trainer\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mforward_transformer(feats, camera_embeddings)\n\u001B[1;32m     24\u001B[0m planes \u001B[38;5;241m=\u001B[39m lrm_trainer\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mreshape_upsample(tokens)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (384) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
